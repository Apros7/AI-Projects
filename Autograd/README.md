
A deep dive into backpropagation to make sure I have it all figured out.

Inspired by Andrej's Karpathy micrograd and George Hotz' tinygrad

Is it computationally expensive? Definitely so. Is it practical to use? Heck no. Should it ever be used in real scenarios? That's a big no. Does it even work? So far :-). 

Can you simple NN to get x% accuracy on MNIST in x minutes on Macbook M1, 16 GB Ram